{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-17T16:06:16.540085Z",
     "start_time": "2025-04-17T16:06:16.533085Z"
    }
   },
   "source": [
    ""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T16:06:28.116744Z",
     "start_time": "2025-04-17T16:06:24.931939Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from aeon.datasets import load_classification\n",
    "from TSTTrainer import TimeSeriesTransformer  # or import the model from the shared module\n",
    "\n"
   ],
   "id": "1aed33ef9df41b3e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique y_train values: ['1' '2' '3' '4' '5' '6' '7' '8' '9']\n",
      "Unique y_test values: ['1' '2' '3' '4' '5' '6' '7' '8' '9']\n",
      "Original X_train shape: (270, 12, 25)\n",
      "Reshaped X_train shape: (270, 25, 12)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T16:06:46.172522800Z",
     "start_time": "2025-04-17T16:06:31.659505Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")          # guarantees no GUI\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot([1, 2, 3], marker=\"o\")\n",
    "plt.savefig(\"smoke.png\", dpi=120)   # < 100 ms even in PyCharm EDU\n",
    "\n",
    "from IPython.display import Image\n",
    "Image(\"smoke.png\")             # shows the saved image in‑line\n"
   ],
   "id": "5161a0651bf4dba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T16:06:18.851549400Z",
     "start_time": "2025-04-17T16:00:10.996461Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_transformer_input(model, instance):\n",
    "    \"\"\"Processes a single instance (shape: (seq_len, input_dim)) through conv layers and adds positional encoding.\"\"\"\n",
    "    x = instance.unsqueeze(0)  # (1, seq_len, input_dim)\n",
    "    x = x.transpose(1, 2)  # (1, input_dim, seq_len)\n",
    "    x = torch.relu(model.bn1(model.conv1(x)))\n",
    "    x = torch.relu(model.bn2(model.conv2(x)))\n",
    "    x = torch.relu(model.bn3(model.conv3(x)))\n",
    "    x = x.transpose(1, 2)  # (1, seq_len, d_model)\n",
    "    x = x + model.positional_encoding\n",
    "    return x\n"
   ],
   "id": "42df1f969aa306ff",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T16:06:18.857539500Z",
     "start_time": "2025-04-17T16:00:11.020458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_denoising_patching(model, clean_instance, corrupted_instance, patch_layer_idx):\n",
    "    \"\"\"\n",
    "    Runs the transformer encoder on clean and corrupted inputs,\n",
    "    then patches the corrupted activation with the clean one at the given layer.\n",
    "    Returns the output logits.\n",
    "    \"\"\"\n",
    "    x_clean = get_transformer_input(model, clean_instance)\n",
    "    x_corr = get_transformer_input(model, corrupted_instance)\n",
    "\n",
    "    # Obtain clean activation at the chosen patch layer.\n",
    "    x_clean_current = x_clean.clone()\n",
    "    for i, layer in enumerate(model.transformer_encoder.layers):\n",
    "        if i == patch_layer_idx:\n",
    "            patch_activation = x_clean_current.clone()\n",
    "        x_clean_current = layer(x_clean_current)\n",
    "\n",
    "    # Run corrupted input and replace activation at patch layer.\n",
    "    x_corr_current = x_corr.clone()\n",
    "    for i, layer in enumerate(model.transformer_encoder.layers):\n",
    "        if i == patch_layer_idx:\n",
    "            x_corr_current = patch_activation.clone()\n",
    "        else:\n",
    "            x_corr_current = layer(x_corr_current)\n",
    "\n",
    "    pooled = model.pool(x_corr_current.transpose(1, 2)).squeeze(-1)\n",
    "    logits = model.classifier(pooled)\n",
    "    return logits"
   ],
   "id": "749f8afae6041e64",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T16:06:18.858039600Z",
     "start_time": "2025-04-17T16:00:11.036974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load test dataset\n",
    "X_train, y_train = load_classification(\"JapaneseVowels\", split=\"train\")\n",
    "X_test, y_test = load_classification(\"JapaneseVowels\", split=\"test\")\n",
    "\n",
    "# Convert labels to integers.\n",
    "y_train = np.array(y_train).astype(np.int64)\n",
    "y_test = np.array(y_test).astype(np.int64)\n",
    "\n",
    "# Process X_test: swap axes as needed.\n",
    "X_test_np = np.swapaxes(X_test.astype(np.float32), 1, 2)\n",
    "min_val = y_train.min()\n",
    "if min_val != 0:\n",
    "    y_test = y_test - min_val\n",
    "X_test_tensor = torch.tensor(X_test_np)\n",
    "y_test_tensor = torch.tensor(y_test)\n",
    "\n",
    "batch_size = 4\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "seq_len = X_test_tensor.shape[1]\n",
    "input_dim = X_test_tensor.shape[2]\n",
    "num_classes = len(np.unique(y_test))"
   ],
   "id": "162b8cee8353b04a",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T16:06:18.858540Z",
     "start_time": "2025-04-17T16:00:11.158957Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the saved model.\n",
    "model = TimeSeriesTransformer(input_dim, num_classes, seq_len, d_model=128, nhead=8,\n",
    "                              num_layers=3, dim_feedforward=256, dropout=0.1).to(device)\n",
    "model_path = \"time_series_transformer_fancy.pth\"\n",
    "if os.path.exists(model_path):\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    print(\"Loaded saved model from\", model_path)\n",
    "else:\n",
    "    print(\"Model file not found. Run training first.\")\n",
    "\n"
   ],
   "id": "d5fc3f47eb8771a8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded saved model from time_series_transformer_fancy.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gebruiker\\AppData\\Local\\Temp\\ipykernel_19416\\501857667.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T16:06:18.859039800Z",
     "start_time": "2025-04-17T16:00:11.204461Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Run inference on test set and collect predictions.\n",
    "results = []\n",
    "with torch.no_grad():\n",
    "    global_index = 0\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        logits = model(X_batch)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        for i in range(X_batch.size(0)):\n",
    "            results.append({\n",
    "                \"index\": global_index,\n",
    "                \"true_label\": y_batch[i].item(),\n",
    "                \"pred_label\": preds[i].item(),\n",
    "                \"confidence\": probs[i].max().item()\n",
    "            })\n",
    "            global_index += 1\n",
    "\n",
    "print(\"Total test instances processed:\", len(results))"
   ],
   "id": "7d7b928ebf0069fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test instances processed: 370\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T16:06:18.859039800Z",
     "start_time": "2025-04-17T16:00:11.703457Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save results as CSV\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"test_results.csv\", index=False)\n",
    "print(\"Test predictions saved to 'test_results.csv'.\")\n"
   ],
   "id": "f234c8f4ffe69f75",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test predictions saved to 'test_results.csv'.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T16:06:18.859540100Z",
     "start_time": "2025-04-17T16:00:11.744957Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert results list to a dictionary keyed by index for fast lookup.\n",
    "def convert_list_to_dict(chosen_correct_index, chosen_incorrect_index):\n",
    "    results_dict = {r[\"index\"]: r for r in results}\n",
    "    if chosen_correct_index not in results_dict or chosen_incorrect_index not in results_dict:\n",
    "        print(\"Chosen indices not found in the results. Please check test_results.csv.\")\n",
    "\n",
    "    return results_dict"
   ],
   "id": "91d06ef01859d846",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T16:06:18.859540100Z",
     "start_time": "2025-04-17T16:00:11.779958Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def load_test_instances(chosen_correct_index, chosen_incorrect_index):\n",
    "\n",
    "    correct_instance = X_test_tensor[chosen_correct_index]\n",
    "    incorrect_instance = X_test_tensor[chosen_incorrect_index]\n",
    "    print(\"Selected correct instance index:\", chosen_correct_index,\n",
    "        \"and incorrect instance index:\", chosen_incorrect_index)\n",
    "\n",
    "    return correct_instance, incorrect_instance"
   ],
   "id": "23cee57fc1fa7f90",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T16:06:18.860040Z",
     "start_time": "2025-04-17T16:00:11.819959Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Manual selection ---\n",
    "# After reviewing test_results.csv, set these indices to the desired instances.\n",
    "# For example:\n",
    "chosen_correct_index = 32  # change this to the index you want (a correct instance)\n",
    "chosen_incorrect_index = 36  # change this to the index you want (a misclassified instance)\n",
    "correct_instance, incorrect_instance = load_test_instances(chosen_correct_index, chosen_incorrect_index)\n",
    "\n",
    "# Choose a transformer encoder layer index to patch.\n",
    "patch_layer_index = 1  # adjust as desired"
   ],
   "id": "57cdc19987a5b9f1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected correct instance index: 32 and incorrect instance index: 36\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T16:06:18.861039900Z",
     "start_time": "2025-04-17T16:00:11.897957Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def summarize_diffs(model, clean_inst, corrupt_inst, device):\n",
    "    \"\"\"\n",
    "    Returns\n",
    "        attn_diff : (layers, heads)  mean |Δ attention| per head\n",
    "        mlp_diff  : (layers,)        mean |Δ MLP output|\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    n_layers = len(model.transformer_encoder.layers)\n",
    "    n_heads  = model.transformer_encoder.layers[0].self_attn.num_heads\n",
    "\n",
    "    # helper – run trunk conv‑BN once\n",
    "    def trunk(x):\n",
    "        x = x.transpose(1, 2)\n",
    "        x = torch.relu(model.bn1(model.conv1(x)))\n",
    "        x = torch.relu(model.bn2(model.conv2(x)))\n",
    "        x = torch.relu(model.bn3(model.conv3(x)))\n",
    "        x = x.transpose(1, 2)\n",
    "        return x + model.positional_encoding\n",
    "\n",
    "    # run both instances and collect summaries layer‑by‑layer\n",
    "    summaries = { \"clean_attn\": [], \"corr_attn\": [],\n",
    "                  \"clean_mlp\":  [], \"corr_mlp\":  [] }\n",
    "\n",
    "    for label, inst in [(\"clean\", clean_inst), (\"corr\", corrupt_inst)]:\n",
    "        x = trunk(inst.to(device).unsqueeze(0))        # (1, L, d_model)\n",
    "\n",
    "        for lyr in model.transformer_encoder.layers:\n",
    "            # ensure we get per‑head matrices\n",
    "            lyr.self_attn.average_attn_weights = False\n",
    "\n",
    "            # --- attention ---\n",
    "            attn_out, attn_w = lyr.self_attn(\n",
    "                x, x, x, need_weights=True, average_attn_weights=False\n",
    "            )                                           # (1, heads, L, L)\n",
    "            head_vec = attn_w.mean(dim=(-1, -2)).squeeze(0)   # (heads,)\n",
    "            summaries[f\"{label}_attn\"].append(head_vec)       # keep on GPU\n",
    "\n",
    "            # residual & norm\n",
    "            x = x + lyr.dropout1(attn_out)\n",
    "            x = lyr.norm1(x)\n",
    "\n",
    "            # --- MLP ---\n",
    "            ff = lyr.linear2(lyr.dropout(lyr.activation(lyr.linear1(x))))  # (1,L,d)\n",
    "            mlp_scalar = ff.abs().mean()                                   # scalar\n",
    "            summaries[f\"{label}_mlp\"].append(mlp_scalar)\n",
    "\n",
    "            # residual & norm\n",
    "            x = x + lyr.dropout2(ff)\n",
    "            x = lyr.norm2(x)\n",
    "\n",
    "    # compute absolute differences\n",
    "    attn_diff = torch.stack([\n",
    "        torch.abs(a - b)                     # (heads,)\n",
    "        for a, b in zip(summaries[\"clean_attn\"], summaries[\"corr_attn\"])\n",
    "    ])                                        # (layers, heads)\n",
    "\n",
    "    mlp_diff = torch.stack([\n",
    "        torch.abs(a - b)\n",
    "        for a, b in zip(summaries[\"clean_mlp\"], summaries[\"corr_mlp\"])\n",
    "    ])                                         # (layers,)\n",
    "\n",
    "    return attn_diff.cpu(), mlp_diff.cpu()"
   ],
   "id": "64904fbca854d03f",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T16:06:18.861539100Z",
     "start_time": "2025-04-17T16:00:11.912457Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_diffs(attn_diff, mlp_diff):\n",
    "    layers, heads = attn_diff.shape\n",
    "    fig, (ax1, ax2) = plt.subplots(\n",
    "        2, 1, figsize=(1.2*heads + 4, 2 + 0.7*layers), gridspec_kw=dict(hspace=0.4)\n",
    "    )\n",
    "\n",
    "    # ---- attention heat‑map ----\n",
    "    sns.heatmap(\n",
    "        attn_diff.detach().numpy(),\n",
    "        ax=ax1, cmap=\"rocket_r\",\n",
    "        xticklabels=[f\"H{h}\" for h in range(heads)],\n",
    "        yticklabels=[f\"L{l}\" for l in range(layers)],\n",
    "        cbar_kws=dict(label=\"mean |Δ attention|\")\n",
    "    )\n",
    "    ax1.set_title(\"Per‑head attention difference  (clean vs corrupt)\")\n",
    "    ax1.set_xlabel(\"Head\");  ax1.set_ylabel(\"Layer\")\n",
    "\n",
    "    # ---- MLP bar chart ----\n",
    "    ax2.bar(np.arange(layers), mlp_diff.numpy())\n",
    "    ax2.set_xticks(np.arange(layers)); ax2.set_xlabel(\"Layer\")\n",
    "    ax2.set_ylabel(\"mean |Δ MLP output|\")\n",
    "    ax2.set_title(\"Per‑layer MLP difference\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "fd470b568ac3c312",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T16:06:18.862056900Z",
     "start_time": "2025-04-17T16:00:11.958473Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_attn_heads_fast(attn_diff, fname=\"attn_diff.png\"):\n",
    "    \"\"\"\n",
    "    • attn_diff  : torch.Tensor  shape (layers, heads)\n",
    "    • writes a tiny PNG -> fname, then returns immediately\n",
    "    \"\"\"\n",
    "    arr = attn_diff.detach().cpu().numpy()      #  (L, H)  ~ 24 numbers\n",
    "    layers, heads = arr.shape\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(1 + 0.6*heads, 0.6*layers), dpi=110)\n",
    "    ax.imshow(arr, aspect='auto', cmap='viridis', origin='upper',\n",
    "              interpolation='nearest')          # no colour‑bar\n",
    "    ax.set_xticks(np.arange(heads))\n",
    "    ax.set_xticklabels([f\"H{h}\" for h in range(heads)], fontsize=8)\n",
    "    ax.set_yticks(np.arange(layers))\n",
    "    ax.set_yticklabels([f\"L{l}\" for l in range(layers)], fontsize=8)\n",
    "    ax.set_xlabel(\"Head\");  ax.set_ylabel(\"Layer\")\n",
    "    ax.set_title(\"mean |Δ attention|  (clean vs corrupt)\", fontsize=10, pad=6)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(fname, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "# ── call it ─────────────────────────────────────────────────────────────\n",
    "# attn_diff, _ = summarize_diffs(model, correct_instance, incorrect_instance, device)\n",
    "# plot_attn_heads_fast(attn_diff)\n",
    "\n",
    "\n"
   ],
   "id": "ee42fdc213ea23e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T16:06:18.877053800Z",
     "start_time": "2025-04-17T15:41:50.637693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# attn_diff, mlp_diff = summarize_diffs(model, correct_instance, incorrect_instance, device)\n",
    "# plot_diffs(attn_diff, mlp_diff)"
   ],
   "id": "834f1c696acc2f4c",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T16:06:18.878038500Z",
     "start_time": "2025-04-17T15:41:50.679192Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    # Run the model normally for the selected instances.\n",
    "    correct_logits = model(correct_instance.unsqueeze(0))\n",
    "    correct_probs = F.softmax(correct_logits, dim=1)\n",
    "    incorrect_logits = model(incorrect_instance.unsqueeze(0))\n",
    "    incorrect_probs = F.softmax(incorrect_logits, dim=1)\n",
    "\n",
    "    # Run the denoising patch experiment: patching the incorrect instance using the correct activation.\n",
    "    patched_logits = run_denoising_patching(model, correct_instance.to(device),\n",
    "                                            incorrect_instance.to(device),\n",
    "                                            patch_layer_index)\n",
    "    patched_probs = F.softmax(patched_logits, dim=1)"
   ],
   "id": "3c299a3dae8382a2",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T16:06:18.881041400Z",
     "start_time": "2025-04-17T15:41:50.732192Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save outputs for later plotting.\n",
    "np.savez(\"patching_results.npz\",\n",
    "         clean_logits=correct_logits.cpu().numpy().flatten(),\n",
    "         corrupted_logits=incorrect_logits.cpu().numpy().flatten(),\n",
    "         patched_logits=patched_logits.cpu().numpy().flatten(),\n",
    "         clean_probs=correct_probs.cpu().numpy().flatten(),\n",
    "         corrupted_probs=incorrect_probs.cpu().numpy().flatten(),\n",
    "         patched_probs=patched_probs.cpu().numpy().flatten(),\n",
    "         num_classes=num_classes)\n",
    "\n",
    "print(\"Patching experiment completed and results saved to 'patching_results.npz'.\")"
   ],
   "id": "6ace511c3a096f92",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patching experiment completed and results saved to 'patching_results.npz'.\n"
     ]
    }
   ],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
